DataModule Loaded: <data_setup.DataModule.DataModule object at 0x1273b5f60>
Model Loaded: EEGNetv4(
  (loss): NLLLoss()
  (acc): MulticlassAccuracy()
  (ensuredims): Ensure4d()
  (dimshuffle): Expression(expression=_transpose_to_b_1_c_0)
  (conv_temporal): Conv2d(1, 64, kernel_size=(1, 64), stride=(1, 1), padding=(0, 32), bias=False)
  (bnorm_temporal): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
  (conv_spatial): Conv2dWithConstraint(64, 128, kernel_size=(64, 1), stride=(1, 1), groups=64, bias=False)
  (bnorm_1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
  (elu_1): Expression(expression=elu)
  (pool_1): MaxPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0, dilation=1, ceil_mode=False)
  (drop_1): Dropout(p=0.25, inplace=False)
  (conv_separable_depth): Conv2d(128, 128, kernel_size=(1, 16), stride=(1, 1), padding=(0, 8), groups=128, bias=False)
  (conv_separable_point): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bnorm_2): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
  (elu_2): Expression(expression=elu)
  (pool_2): MaxPool2d(kernel_size=(1, 8), stride=(1, 8), padding=0, dilation=1, ceil_mode=False)
  (drop_2): Dropout(p=0.25, inplace=False)
  (conv_classifier): Conv2d(128, 6, kernel_size=(1, 128), stride=(1, 1))
  (softmax): LogSoftmax(dim=1)
  (permute_back): Expression(expression=_transpose_1_0)
  (squeeze): Expression(expression=squeeze_final_output)
)
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/Users/arnavkapur/.pyenv/versions/3.10.12/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
Data shape: torch.Size([374, 64, 4096]), Labels shape: torch.Size([468])
Data shape: torch.Size([47, 64, 4096]), Labels shape: torch.Size([468])
Sanity Checking: |                                                                                                  | 0/? [00:00<?, ?it/s]
   | Name                 | Type                 | Params | Mode
-----------------------------------------------------------------------
0  | loss                 | NLLLoss              | 0      | train
1  | acc                  | MulticlassAccuracy   | 0      | train
2  | ensuredims           | Ensure4d             | 0      | train
3  | dimshuffle           | Expression           | 0      | train
4  | conv_temporal        | Conv2d               | 4.1 K  | train
5  | bnorm_temporal       | BatchNorm2d          | 128    | train
6  | conv_spatial         | Conv2dWithConstraint | 8.2 K  | train
7  | bnorm_1              | BatchNorm2d          | 256    | train
8  | elu_1                | Expression           | 0      | train
9  | pool_1               | MaxPool2d            | 0      | train
10 | drop_1               | Dropout              | 0      | train
11 | conv_separable_depth | Conv2d               | 2.0 K  | train
12 | conv_separable_point | Conv2d               | 16.4 K | train
13 | bnorm_2              | BatchNorm2d          | 256    | train
14 | elu_2                | Expression           | 0      | train
15 | pool_2               | MaxPool2d            | 0      | train
16 | drop_2               | Dropout              | 0      | train
17 | conv_classifier      | Conv2d               | 98.3 K | train
18 | softmax              | LogSoftmax           | 0      | train
19 | permute_back         | Expression           | 0      | train
20 | squeeze              | Expression           | 0      | train
-----------------------------------------------------------------------
129 K     Trainable params
0         Non-trainable params
129 K     Total params
0.519     Total estimated model params size (MB)
21        Modules in train mode
0         Modules in eval mode
/Users/arnavkapur/.pyenv/versions/3.10.12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.